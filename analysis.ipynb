{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Classification — Analysis\n",
    "\n",
    "This notebook has **two parts per label**:\n",
    "\n",
    "1. **Pre-training** — dataset statistics, class balance, text characteristics, co-occurrence.\n",
    "2. **Post-training** — grid search results, test metrics, confusion matrices, error analysis.\n",
    "\n",
    "Run the cells top-to-bottom. The post-training sections load from `results/` and will be skipped if the pipeline hasn't been run yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\nfrom training.config import (\n    DATASET_PATH,\n    DATASET_SHEET,\n    FINAL_EVAL_CSV,\n    GRID_SEARCH_CSV,\n    LABEL_TRUE_VALUES,\n    LABELS,\n    PREDICTIONS_SUBDIR,\n    RESULTS_DIR,\n    TEXT_COLUMN,\n)\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\npd.set_option(\"display.max_colwidth\", 120)\n\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = os.path.splitext(DATASET_PATH)[1].lower()\n",
    "if ext == \".xlsx\":\n",
    "    df = pd.read_excel(DATASET_PATH, sheet_name=DATASET_SHEET)\n",
    "else:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "for label in LABELS:\n",
    "    df[label] = df[label].apply(lambda v: v in LABEL_TRUE_VALUES)\n",
    "\n",
    "df = df.dropna(subset=[TEXT_COLUMN]).reset_index(drop=True)\n",
    "df[\"text_len\"] = df[TEXT_COLUMN].str.len()\n",
    "df[\"word_count\"] = df[TEXT_COLUMN].str.split().str.len()\n",
    "df[\"n_labels\"] = df[LABELS].sum(axis=1).astype(int)\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Results (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = os.path.join(RESULTS_DIR, GRID_SEARCH_CSV)\n",
    "eval_path = os.path.join(RESULTS_DIR, FINAL_EVAL_CSV)\n",
    "\n",
    "HAS_RESULTS = os.path.isfile(eval_path)\n",
    "\n",
    "if HAS_RESULTS:\n",
    "    grid_df = pd.read_csv(grid_path) if os.path.isfile(grid_path) else None\n",
    "    eval_df = pd.read_csv(eval_path)\n",
    "    preds = {}\n",
    "    for label in LABELS:\n",
    "        p = os.path.join(RESULTS_DIR, PREDICTIONS_SUBDIR, f\"{label}_predictions.csv\")\n",
    "        if os.path.isfile(p):\n",
    "            preds[label] = pd.read_csv(p)\n",
    "    print(f\"Results loaded: {len(eval_df)} labels evaluated.\")\n",
    "    if grid_df is not None:\n",
    "        print(f\"Grid search: {len(grid_df)} trials.\")\n",
    "else:\n",
    "    grid_df = None\n",
    "    eval_df = None\n",
    "    preds = {}\n",
    "    print(\"No results found — post-training sections will be skipped.\")\n",
    "    print(\"Run `python train.py` first, then re-run this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Global Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[LABELS].sum().sort_values(ascending=False)\n",
    "pcts = (counts / len(df) * 100).round(1)\n",
    "\n",
    "summary = pd.DataFrame({\"count\": counts, \"pct\": pcts})\n",
    "print(summary.to_string())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "bars = ax.barh(counts.index[::-1], counts.values[::-1])\n",
    "for bar, pct in zip(bars, pcts.values[::-1]):\n",
    "    ax.text(\n",
    "        bar.get_width() + len(df) * 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{pct}%\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "ax.set_xlabel(\"Count\")\n",
    "ax.set_title(\"Label Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels per Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "df[\"n_labels\"].value_counts().sort_index().plot.bar(ax=ax)\n",
    "ax.set_xlabel(\"Number of labels per example\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Multi-label Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = pd.DataFrame(index=LABELS, columns=LABELS, dtype=int)\n",
    "for l1 in LABELS:\n",
    "    for l2 in LABELS:\n",
    "        co.loc[l1, l2] = int((df[l1] & df[l2]).sum())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sns.heatmap(co.astype(int), annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax, square=True)\n",
    "ax.set_title(\"Label Co-occurrence\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Length Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df[\"text_len\"].hist(bins=50, ax=axes[0], edgecolor=\"white\")\n",
    "axes[0].set_xlabel(\"Character length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Text Length (characters)\")\n",
    "axes[0].axvline(\n",
    "    df[\"text_len\"].median(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"median={df['text_len'].median():.0f}\",\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "df[\"word_count\"].hist(bins=50, ax=axes[1], edgecolor=\"white\")\n",
    "axes[1].set_xlabel(\"Word count\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Text Length (words)\")\n",
    "axes[1].axvline(\n",
    "    df[\"word_count\"].median(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"median={df['word_count'].median():.0f}\",\n",
    ")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Test Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if HAS_RESULTS and eval_df is not None:\n    display_cols = [\n        c\n        for c in [\n            \"label\",\n            \"test_f1\",\n            \"test_precision\",\n            \"test_recall\",\n            \"test_accuracy\",\n            \"n_test\",\n            \"n_test_positive\",\n            \"gen_model\",\n            \"reflection_model\",\n        ]\n        if c in eval_df.columns\n    ]\n    display(eval_df[display_cols])\n\n    fig, ax = plt.subplots(figsize=(8, 4))\n    metrics = eval_df.set_index(\"label\")[[\"test_f1\", \"test_precision\", \"test_recall\"]]\n    metrics.plot.bar(ax=ax)\n    ax.set_ylabel(\"Score\")\n    ax.set_title(\"Test Metrics by Label\")\n    ax.set_ylim(0, 1.05)\n    ax.legend(loc=\"lower right\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Skipped — no results.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Per-Label Analysis\n",
    "\n",
    "Each label section below contains:\n",
    "- **Pre-training**: class balance, text length by class, sample examples.\n",
    "- **Post-training**: grid search heatmap, test confusion matrix, error examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def pre_training_analysis(label):\n    \"\"\"Pre-training statistics for a single label.\"\"\"\n    pos = df[df[label]]\n    neg = df[~df[label]]\n    n_pos, n_neg = len(pos), len(neg)\n    print(\n        f\"Positive: {n_pos} ({n_pos / len(df) * 100:.1f}%)  |  \"\n        f\"Negative: {n_neg} ({n_neg / len(df) * 100:.1f}%)\"\n    )\n    print(f\"Imbalance ratio: 1:{n_neg / n_pos:.1f}\" if n_pos > 0 else \"No positives!\")\n    print()\n\n    # Text length comparison\n    fig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\n    for ax, col, title in [\n        (axes[0], \"text_len\", \"Char length\"),\n        (axes[1], \"word_count\", \"Word count\"),\n    ]:\n        bins = np.linspace(df[col].min(), df[col].quantile(0.95), 40)\n        ax.hist(neg[col], bins=bins, alpha=0.6, label=\"Negative\", edgecolor=\"white\")\n        ax.hist(pos[col], bins=bins, alpha=0.6, label=\"Positive\", edgecolor=\"white\")\n        ax.set_xlabel(title)\n        ax.set_ylabel(\"Count\")\n        ax.legend()\n        ax.set_title(f\"{title} — positive vs negative\")\n    plt.suptitle(f\"'{label}' — text length distributions\", fontsize=12, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n    # Co-occurring labels for positives\n    if n_pos > 0:\n        other_labels = [lb for lb in LABELS if lb != label]\n        co_counts = pos[other_labels].sum().sort_values(ascending=False)\n        co_pcts = (co_counts / n_pos * 100).round(1)\n        print(\"Co-occurring labels among positives:\")\n        for name, count in co_counts.items():\n            if count > 0:\n                print(f\"  {name}: {count} ({co_pcts[name]}%)\")\n        print()\n\n    # Sample examples\n    n_show = min(3, n_pos)\n    if n_show > 0:\n        print(f\"Sample positive examples (n={n_show}):\")\n        for _, row in pos.sample(n=n_show, random_state=42).iterrows():\n            print(f\"  - {row[TEXT_COLUMN][:200]}\")\n    print()\n\n\ndef post_training_analysis(label):\n    \"\"\"Post-training results for a single label.\"\"\"\n    if not HAS_RESULTS or eval_df is None:\n        print(\"Skipped — no results.\")\n        return\n\n    # Grid search heatmap\n    if grid_df is not None and label in grid_df[\"label\"].values:\n        label_grid = grid_df[grid_df[\"label\"] == label]\n        pivot = label_grid.pivot_table(\n            index=\"gen_model\", columns=\"reflection_model\", values=\"val_f1\"\n        )\n        fig, ax = plt.subplots(figsize=(7, 4))\n        sns.heatmap(\n            pivot,\n            annot=True,\n            fmt=\".3f\",\n            cmap=\"YlOrRd\",\n            ax=ax,\n            vmin=0,\n            vmax=1,\n            square=True,\n        )\n        ax.set_title(f\"'{label}' — Grid Search Val F1\")\n        plt.tight_layout()\n        plt.show()\n\n    # Test metrics\n    row = eval_df[eval_df[\"label\"] == label]\n    if row.empty:\n        print(f\"No evaluation row for '{label}'.\")\n        return\n    row = row.iloc[0]\n    print(f\"Test F1:        {row['test_f1']:.4f}\")\n    print(f\"Test Precision: {row['test_precision']:.4f}\")\n    print(f\"Test Recall:    {row['test_recall']:.4f}\")\n    print(f\"Test Accuracy:  {row['test_accuracy']:.4f}\")\n    print(\n        f\"Test size:      {int(row['n_test'])} ({int(row['n_test_positive'])} positive)\"\n    )\n    if \"gen_model\" in row.index:\n        print(f\"Best gen model:        {row['gen_model']}\")\n        print(f\"Best reflection model: {row['reflection_model']}\")\n    print()\n\n    # Confusion matrix\n    if label in preds:\n        pred_df = preds[label]\n        y_true = (pred_df[\"gold\"] == \"true\").astype(int)\n        y_pred = (pred_df[\"predicted\"] == \"true\").astype(int)\n        cm = confusion_matrix(y_true, y_pred)\n\n        fig, ax = plt.subplots(figsize=(4, 4))\n        ConfusionMatrixDisplay(cm, display_labels=[\"Negative\", \"Positive\"]).plot(\n            ax=ax, cmap=\"Blues\"\n        )\n        ax.set_title(f\"'{label}' — Test Confusion Matrix\")\n        plt.tight_layout()\n        plt.show()\n\n        # Error examples\n        errors = pred_df[~pred_df[\"correct\"]]\n        if len(errors) > 0:\n            fn = errors[errors[\"gold\"] == \"true\"]\n            fp = errors[errors[\"predicted\"] == \"true\"]\n            print(f\"Errors: {len(errors)} total — {len(fp)} FP, {len(fn)} FN\")\n            for err_type, err_df in [(\"False Positive\", fp), (\"False Negative\", fn)]:\n                n_show = min(3, len(err_df))\n                if n_show > 0:\n                    print(f\"\\n  {err_type} examples (n={n_show}):\")\n                    for _, r in err_df.head(n_show).iterrows():\n                        print(f\"    - {r['text'][:200]}\")\n        else:\n            print(\"No errors on the test set!\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### romantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — romantic\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"romantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — romantic\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"romantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — family\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — family\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### friendship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — friendship\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"friendship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — friendship\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"friendship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### professional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — professional\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"professional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — professional\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"professional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — unknown\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — unknown\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — irrelevant\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"irrelevant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — irrelevant\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"irrelevant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}