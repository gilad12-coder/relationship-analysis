{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Classification — Analysis\n",
    "\n",
    "This notebook is split into two phases:\n",
    "\n",
    "- **Part 1 — Pre-training** (cells up to \"Part 2\"): dataset statistics, class balance, text characteristics, co-occurrence. Run these **before** training to understand your data.\n",
    "- **Part 2 — Post-training** (everything after): grid search heatmaps, test metrics, confusion matrices, error analysis. Run these **after** `train.ipynb` (or `python train.py`) has completed.\n",
    "\n",
    "Post-training cells gracefully skip if no results exist yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:44.658679Z",
     "start_time": "2026-02-23T15:45:44.436017Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from training.config import (\n",
    "    DATASET_PATH,\n",
    "    DATASET_SHEET,\n",
    "    EVAL_CSV,\n",
    "    GRID_SEARCH_CSV,\n",
    "    LABEL_TRUE_VALUES,\n",
    "    LABELS,\n",
    "    PREDICTIONS_SUBDIR,\n",
    "    RESULTS_DIR,\n",
    "    TEXT_COLUMN,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:44.697127Z",
     "start_time": "2026-02-23T15:45:44.659707Z"
    }
   },
   "outputs": [],
   "source": [
    "ext = os.path.splitext(DATASET_PATH)[1].lower()\n",
    "if ext == \".xlsx\":\n",
    "    df = pd.read_excel(DATASET_PATH, sheet_name=DATASET_SHEET)\n",
    "else:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "for label in LABELS:\n",
    "    df[label] = df[label].apply(lambda v: v in LABEL_TRUE_VALUES)\n",
    "\n",
    "df = df.dropna(subset=[TEXT_COLUMN]).reset_index(drop=True)\n",
    "df[\"text_len\"] = df[TEXT_COLUMN].str.len()\n",
    "df[\"word_count\"] = df[TEXT_COLUMN].str.split().str.len()\n",
    "df[\"n_labels\"] = df[LABELS].sum(axis=1).astype(int)\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Results (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:44.728376Z",
     "start_time": "2026-02-23T15:45:44.701172Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_path = os.path.join(RESULTS_DIR, EVAL_CSV)\n",
    "grid_path = os.path.join(RESULTS_DIR, GRID_SEARCH_CSV)\n",
    "\n",
    "HAS_RESULTS = os.path.isfile(eval_path)\n",
    "\n",
    "if HAS_RESULTS:\n",
    "    full_eval_df = pd.read_csv(eval_path)\n",
    "    eval_df = full_eval_df[full_eval_df[\"stage\"] == \"optimized\"].reset_index(drop=True)\n",
    "    baseline_df = full_eval_df[full_eval_df[\"stage\"] == \"baseline\"].reset_index(drop=True)\n",
    "    HAS_BASELINE = len(baseline_df) > 0\n",
    "    grid_df = pd.read_csv(grid_path) if os.path.isfile(grid_path) else None\n",
    "\n",
    "    preds, baseline_preds = {}, {}\n",
    "    for label in LABELS:\n",
    "        for stage, store in [(\"optimized\", preds), (\"baseline\", baseline_preds)]:\n",
    "            p = os.path.join(RESULTS_DIR, PREDICTIONS_SUBDIR, f\"{stage}_{label}_predictions.csv\")\n",
    "            if os.path.isfile(p):\n",
    "                store[label] = pd.read_csv(p)\n",
    "\n",
    "    print(f\"Results loaded: {len(eval_df)} optimized, {len(baseline_df)} baseline.\")\n",
    "    if grid_df is not None:\n",
    "        print(f\"Grid search: {len(grid_df)} trials.\")\n",
    "else:\n",
    "    grid_df = eval_df = baseline_df = None\n",
    "    HAS_BASELINE = False\n",
    "    preds, baseline_preds = {}, {}\n",
    "    print(\"No results found — post-training sections will be skipped.\")\n",
    "    print(\"Run `python train.py` first, then re-run this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Part 1: Pre-Training Analysis\n",
    "\n",
    "Run these cells **before** training to understand the dataset.\n",
    "\n",
    "---\n",
    "## Global Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:44.835885Z",
     "start_time": "2026-02-23T15:45:44.731733Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = df[LABELS].sum().sort_values(ascending=False)\n",
    "pcts = (counts / len(df) * 100).round(1)\n",
    "\n",
    "summary = pd.DataFrame({\"count\": counts, \"pct\": pcts})\n",
    "print(summary.to_string())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "bars = ax.barh(counts.index[::-1], counts.values[::-1])\n",
    "for bar, pct in zip(bars, pcts.values[::-1]):\n",
    "    ax.text(\n",
    "        bar.get_width() + len(df) * 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{pct}%\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "ax.set_xlabel(\"Count\")\n",
    "ax.set_title(\"Label Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:44.975083Z",
     "start_time": "2026-02-23T15:45:44.844276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate labels across rows sharing the same text\n",
    "df_agg = df.groupby(TEXT_COLUMN)[LABELS].any().reset_index()\n",
    "df_agg[\"n_labels\"] = df_agg[LABELS].sum(axis=1).astype(int)\n",
    "\n",
    "n_unique = len(df_agg)\n",
    "n_multi = (df_agg[\"n_labels\"] > 1).sum()\n",
    "print(f\"Unique texts: {n_unique} | Multi-label: {n_multi} ({n_multi / n_unique * 100:.1f}%)\")\n",
    "\n",
    "# Labels per unique text\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "df_agg[\"n_labels\"].value_counts().sort_index().plot.bar(ax=ax)\n",
    "ax.set_xlabel(\"Number of labels per text\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Label Count per Unique Text (aggregated across rows)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Label pair co-occurrence (sorted bar chart)\n",
    "from itertools import combinations\n",
    "\n",
    "pair_counts = {}\n",
    "for l1, l2 in combinations(LABELS, 2):\n",
    "    count = int((df_agg[l1] & df_agg[l2]).sum())\n",
    "    if count > 0:\n",
    "        pair_counts[f\"{l1} + {l2}\"] = count\n",
    "\n",
    "if pair_counts:\n",
    "    pairs_sorted = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    pair_labels, pair_values = zip(*pairs_sorted)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(3, len(pair_labels) * 0.4)))\n",
    "    bars = ax.barh(pair_labels[::-1], pair_values[::-1], color=\"#5C6BC0\")\n",
    "    for bar in bars:\n",
    "        w = bar.get_width()\n",
    "        ax.text(w + 0.3, bar.get_y() + bar.get_height() / 2,\n",
    "                str(int(w)), va=\"center\", fontsize=10)\n",
    "    ax.set_xlabel(\"Texts with both labels\")\n",
    "    ax.set_title(\"Label Pair Overlap (texts tagged with both)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No label co-occurrences found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Length Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:45.195569Z",
     "start_time": "2026-02-23T15:45:44.977404Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df[\"text_len\"].hist(bins=50, ax=axes[0], edgecolor=\"white\")\n",
    "axes[0].set_xlabel(\"Character length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Text Length (characters)\")\n",
    "axes[0].axvline(\n",
    "    df[\"text_len\"].median(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"median={df['text_len'].median():.0f}\",\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "df[\"word_count\"].hist(bins=50, ax=axes[1], edgecolor=\"white\")\n",
    "axes[1].set_xlabel(\"Word count\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Text Length (words)\")\n",
    "axes[1].axvline(\n",
    "    df[\"word_count\"].median(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"median={df['word_count'].median():.0f}\",\n",
    ")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSPy Split Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:45.428940Z",
     "start_time": "2026-02-23T15:45:45.197995Z"
    }
   },
   "outputs": [],
   "source": [
    "from training.data import split_pipeline\n",
    "from loguru import logger\n",
    "\n",
    "logger.disable(\"training\")\n",
    "splits = split_pipeline.run(df)\n",
    "logger.enable(\"training\")\n",
    "\n",
    "rows = []\n",
    "for label in LABELS:\n",
    "    ls = splits[label]\n",
    "    rows.append({\n",
    "        \"label\": label,\n",
    "        \"train_pos\": int(ls.dspy.train[label].sum()),\n",
    "        \"train_neg\": int((~ls.dspy.train[label]).sum()),\n",
    "        \"val_pos\": int(ls.dspy.val[label].sum()),\n",
    "        \"val_neg\": int((~ls.dspy.val[label]).sum()),\n",
    "        \"holdout_pos\": int(ls.holdout[label].sum()),\n",
    "        \"holdout_neg\": int((~ls.holdout[label]).sum()),\n",
    "    })\n",
    "\n",
    "split_df = pd.DataFrame(rows)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "x = np.arange(len(split_df))\n",
    "w = 0.25\n",
    "\n",
    "# Positives\n",
    "for i, (col, lbl, color) in enumerate([\n",
    "    (\"train_pos\", \"Train\", \"#2196F3\"),\n",
    "    (\"val_pos\", \"Val\", \"#FF9800\"),\n",
    "    (\"holdout_pos\", \"Holdout\", \"#4CAF50\"),\n",
    "]):\n",
    "    offset = (i - 1) * w\n",
    "    rects = axes[0].bar(x + offset, split_df[col], w, label=lbl, color=color)\n",
    "    for rect in rects:\n",
    "        h = rect.get_height()\n",
    "        if h > 0:\n",
    "            axes[0].text(rect.get_x() + rect.get_width() / 2, h + 0.3,\n",
    "                         str(int(h)), ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(split_df[\"label\"], rotation=45, ha=\"right\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Positive Examples per Split\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Negatives\n",
    "for i, (col, lbl, color) in enumerate([\n",
    "    (\"train_neg\", \"Train\", \"#2196F3\"),\n",
    "    (\"val_neg\", \"Val\", \"#FF9800\"),\n",
    "    (\"holdout_neg\", \"Holdout\", \"#4CAF50\"),\n",
    "]):\n",
    "    offset = (i - 1) * w\n",
    "    rects = axes[1].bar(x + offset, split_df[col], w, label=lbl, color=color)\n",
    "    for rect in rects:\n",
    "        h = rect.get_height()\n",
    "        if h > 0:\n",
    "            axes[1].text(rect.get_x() + rect.get_width() / 2, h + 0.3,\n",
    "                         str(int(h)), ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(split_df[\"label\"], rotation=45, ha=\"right\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Negative Examples per Split\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle(\"DSPy Splits per Label (deterministic — matches training)\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Part 2: Post-Training Analysis\n",
    "\n",
    "Run these cells **after** training has completed (`train.ipynb` or `python train.py`).\n",
    "Results are loaded from `training/results/`.\n",
    "\n",
    "---\n",
    "### Global Holdout Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:45.549585Z",
     "start_time": "2026-02-23T15:45:45.431638Z"
    }
   },
   "outputs": [],
   "source": [
    "if HAS_RESULTS and eval_df is not None:\n",
    "    display_cols = [\n",
    "        c\n",
    "        for c in [\n",
    "            \"label\",\n",
    "            \"holdout_f1\",\n",
    "            \"holdout_precision\",\n",
    "            \"holdout_recall\",\n",
    "            \"holdout_accuracy\",\n",
    "            \"n_holdout\",\n",
    "            \"n_holdout_positive\",\n",
    "            \"gen_model\",\n",
    "            \"reflection_model\",\n",
    "        ]\n",
    "        if c in eval_df.columns\n",
    "    ]\n",
    "    print(\"=== Optimised Results ===\")\n",
    "    display(eval_df[display_cols])\n",
    "\n",
    "    if HAS_BASELINE and baseline_df is not None:\n",
    "        baseline_cols = [\n",
    "            c\n",
    "            for c in [\"label\", \"holdout_f1\", \"holdout_precision\", \"holdout_recall\", \"holdout_accuracy\", \"gen_model\"]\n",
    "            if c in baseline_df.columns\n",
    "        ]\n",
    "        print(\"\\n=== Baseline Results ===\")\n",
    "        display(baseline_df[baseline_cols])\n",
    "\n",
    "        # Side-by-side comparison with P/R/F1\n",
    "        comparison = eval_df[[\"label\", \"holdout_f1\", \"holdout_precision\", \"holdout_recall\"]].copy()\n",
    "        comparison = comparison.rename(columns={\n",
    "            \"holdout_f1\": \"opt_f1\", \"holdout_precision\": \"opt_P\", \"holdout_recall\": \"opt_R\"\n",
    "        })\n",
    "        bl = baseline_df[[\"label\", \"holdout_f1\", \"holdout_precision\", \"holdout_recall\"]].copy()\n",
    "        bl = bl.rename(columns={\n",
    "            \"holdout_f1\": \"base_f1\", \"holdout_precision\": \"base_P\", \"holdout_recall\": \"base_R\"\n",
    "        })\n",
    "        comparison = comparison.merge(bl, on=\"label\", how=\"outer\")\n",
    "        comparison[\"f1_delta\"] = comparison[\"opt_f1\"] - comparison[\"base_f1\"]\n",
    "        print(\"\\n=== Baseline vs Optimised ===\")\n",
    "        display(comparison[[\"label\", \"base_f1\", \"base_P\", \"base_R\", \"opt_f1\", \"opt_P\", \"opt_R\", \"f1_delta\"]])\n",
    "\n",
    "        # Grouped bar chart (F1 only — more bars would be unreadable)\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        x = np.arange(len(comparison))\n",
    "        w = 0.35\n",
    "        ax.bar(x - w / 2, comparison[\"base_f1\"], w, label=\"Baseline F1\")\n",
    "        ax.bar(x + w / 2, comparison[\"opt_f1\"], w, label=\"Optimised F1\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(comparison[\"label\"], rotation=45, ha=\"right\")\n",
    "        ax.set_ylabel(\"F1 Score\")\n",
    "        ax.set_title(\"Baseline vs Optimised — Holdout F1 by Label\")\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        metrics = eval_df.set_index(\"label\")[[\"holdout_f1\", \"holdout_precision\", \"holdout_recall\"]]\n",
    "        metrics.plot.bar(ax=ax)\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_title(\"Holdout Metrics by Label\")\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Skipped — no results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Per-Label Deep Dive\n",
    "\n",
    "Each label below has two cells:\n",
    "1. **Pre-training** — class balance, text length by class, sample examples. Safe to run anytime.\n",
    "2. **Post-training** — grid search heatmap, test confusion matrix, error examples. Requires training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:45.624304Z",
     "start_time": "2026-02-23T15:45:45.553486Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_training_analysis(label):\n",
    "    \"\"\"Pre-training statistics for a single label.\"\"\"\n",
    "    pos = df[df[label]]\n",
    "    neg = df[~df[label]]\n",
    "    n_pos, n_neg = len(pos), len(neg)\n",
    "    print(\n",
    "        f\"Positive: {n_pos} ({n_pos / len(df) * 100:.1f}%)  |  \"\n",
    "        f\"Negative: {n_neg} ({n_neg / len(df) * 100:.1f}%)\"\n",
    "    )\n",
    "    print(f\"Imbalance ratio: 1:{n_neg / n_pos:.1f}\" if n_pos > 0 else \"No positives!\")\n",
    "    print()\n",
    "\n",
    "    # Text length comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\n",
    "    for ax, col, title in [\n",
    "        (axes[0], \"text_len\", \"Char length\"),\n",
    "        (axes[1], \"word_count\", \"Word count\"),\n",
    "    ]:\n",
    "        bins = np.linspace(df[col].min(), df[col].quantile(0.95), 40)\n",
    "        ax.hist(neg[col], bins=bins, alpha=0.6, label=\"Negative\", edgecolor=\"white\")\n",
    "        ax.hist(pos[col], bins=bins, alpha=0.6, label=\"Positive\", edgecolor=\"white\")\n",
    "        ax.set_xlabel(title)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{title} — positive vs negative\")\n",
    "    plt.suptitle(f\"'{label}' — text length distributions\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Co-occurring labels (using aggregated data)\n",
    "    agg_pos = df_agg[df_agg[label]]\n",
    "    if len(agg_pos) > 0:\n",
    "        other_labels = [lb for lb in LABELS if lb != label]\n",
    "        co_counts = agg_pos[other_labels].sum().sort_values(ascending=False)\n",
    "        co_pcts = (co_counts / len(agg_pos) * 100).round(1)\n",
    "        has_co = co_counts.sum() > 0\n",
    "        if has_co:\n",
    "            print(\"Co-occurring labels among positive texts:\")\n",
    "            for name, count in co_counts.items():\n",
    "                if count > 0:\n",
    "                    print(f\"  {name}: {int(count)} ({co_pcts[name]}%)\")\n",
    "        else:\n",
    "            print(\"No co-occurring labels.\")\n",
    "        print()\n",
    "\n",
    "    # Sample examples\n",
    "    n_show = min(3, n_pos)\n",
    "    if n_show > 0:\n",
    "        print(f\"Sample positive examples (n={n_show}):\")\n",
    "        for _, row in pos.sample(n=n_show, random_state=42).iterrows():\n",
    "            print(f\"  - {row[TEXT_COLUMN][:200]}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def _to_binary(series):\n",
    "    \"\"\"Convert a gold/predicted column to int (1=positive, 0=negative).\n",
    "\n",
    "    Handles both string ('true'/'false') and boolean (True/False) values\n",
    "    since pandas may auto-convert CSV values to booleans.\n",
    "    \"\"\"\n",
    "    return series.apply(lambda v: str(v).strip().lower() == \"true\").astype(int)\n",
    "\n",
    "\n",
    "def post_training_analysis(label):\n",
    "    \"\"\"Post-training results for a single label.\"\"\"\n",
    "    if not HAS_RESULTS or eval_df is None:\n",
    "        print(\"Skipped — no results.\")\n",
    "        return\n",
    "\n",
    "    # Grid search heatmap\n",
    "    if grid_df is not None and label in grid_df[\"label\"].values:\n",
    "        label_grid = grid_df[grid_df[\"label\"] == label]\n",
    "        metric_col = \"holdout_f1\" if \"holdout_f1\" in label_grid.columns else \"val_f1\"\n",
    "        pivot = label_grid.pivot_table(\n",
    "            index=\"gen_model\", columns=\"reflection_model\", values=metric_col\n",
    "        )\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "        sns.heatmap(\n",
    "            pivot,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            cmap=\"YlOrRd\",\n",
    "            ax=ax,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            square=True,\n",
    "        )\n",
    "        ax.set_title(f\"'{label}' — Grid Search {metric_col.replace('_', ' ').title()}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Baseline vs Optimised metrics\n",
    "    row = eval_df[eval_df[\"label\"] == label]\n",
    "    if row.empty:\n",
    "        print(f\"No evaluation row for '{label}'.\")\n",
    "        return\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    if HAS_BASELINE and baseline_df is not None:\n",
    "        bl_row = baseline_df[baseline_df[\"label\"] == label]\n",
    "        if not bl_row.empty:\n",
    "            bl_row = bl_row.iloc[0]\n",
    "            print(f\"{'Metric':<12} {'Baseline':>10} {'Optimised':>10}\")\n",
    "            print(f\"{'-'*12} {'-'*10} {'-'*10}\")\n",
    "            print(f\"{'F1':<12} {bl_row['holdout_f1']:>10.4f} {row['holdout_f1']:>10.4f}\")\n",
    "            print(f\"{'Precision':<12} {bl_row['holdout_precision']:>10.4f} {row['holdout_precision']:>10.4f}\")\n",
    "            print(f\"{'Recall':<12} {bl_row['holdout_recall']:>10.4f} {row['holdout_recall']:>10.4f}\")\n",
    "            print(f\"{'Accuracy':<12} {bl_row['holdout_accuracy']:>10.4f} {row['holdout_accuracy']:>10.4f}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"F1:        {row['holdout_f1']:.4f}\")\n",
    "        print(f\"Precision: {row['holdout_precision']:.4f}\")\n",
    "        print(f\"Recall:    {row['holdout_recall']:.4f}\")\n",
    "        print(f\"Accuracy:  {row['holdout_accuracy']:.4f}\")\n",
    "        print()\n",
    "\n",
    "    print(\n",
    "        f\"Holdout size: {int(row['n_holdout'])} ({int(row['n_holdout_positive'])} positive)\"\n",
    "    )\n",
    "    if \"gen_model\" in row.index:\n",
    "        print(f\"Generation model:  {row['gen_model']}\")\n",
    "        print(f\"Reflection model:  {row['reflection_model']}\")\n",
    "    print()\n",
    "\n",
    "    # Confusion matrix\n",
    "    if label in preds:\n",
    "        pred_df = preds[label]\n",
    "        y_true = _to_binary(pred_df[\"gold\"])\n",
    "        y_pred = _to_binary(pred_df[\"predicted\"])\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ConfusionMatrixDisplay(cm, display_labels=[\"Negative\", \"Positive\"]).plot(\n",
    "            ax=ax, cmap=\"Blues\"\n",
    "        )\n",
    "        ax.set_title(f\"'{label}' — Holdout Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Error examples\n",
    "        errors = pred_df[~pred_df[\"correct\"]]\n",
    "        if len(errors) > 0:\n",
    "            fn = errors[_to_binary(errors[\"gold\"]) == 1]\n",
    "            fp = errors[_to_binary(errors[\"predicted\"]) == 1]\n",
    "            print(f\"Errors: {len(errors)} total — {len(fp)} FP, {len(fn)} FN\")\n",
    "            for err_type, err_df in [(\"False Positive\", fp), (\"False Negative\", fn)]:\n",
    "                n_show = min(3, len(err_df))\n",
    "                if n_show > 0:\n",
    "                    print(f\"\\n  {err_type} examples (n={n_show}):\")\n",
    "                    for _, r in err_df.head(n_show).iterrows():\n",
    "                        print(f\"    - {r['text'][:200]}\")\n",
    "        else:\n",
    "            print(\"No errors on the holdout set!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### romantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:45.846002Z",
     "start_time": "2026-02-23T15:45:45.625805Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — romantic\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"romantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:45.924427Z",
     "start_time": "2026-02-23T15:45:45.857397Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — romantic\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"romantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:46.159990Z",
     "start_time": "2026-02-23T15:45:45.933343Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — family\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:46.246674Z",
     "start_time": "2026-02-23T15:45:46.169104Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — family\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### friendship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:46.486652Z",
     "start_time": "2026-02-23T15:45:46.259065Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — friendship\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"friendship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:46.564761Z",
     "start_time": "2026-02-23T15:45:46.488886Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — friendship\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"friendship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### professional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:46.796733Z",
     "start_time": "2026-02-23T15:45:46.574420Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — professional\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"professional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:46.955771Z",
     "start_time": "2026-02-23T15:45:46.807256Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — professional\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"professional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:47.187187Z",
     "start_time": "2026-02-23T15:45:46.958603Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — unknown\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:47.273199Z",
     "start_time": "2026-02-23T15:45:47.195918Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — unknown\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:47.540832Z",
     "start_time": "2026-02-23T15:45:47.282464Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING — irrelevant\")\n",
    "print(\"=\" * 60)\n",
    "pre_training_analysis(\"irrelevant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T15:45:47.697781Z",
     "start_time": "2026-02-23T15:45:47.549722Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING — irrelevant\")\n",
    "print(\"=\" * 60)\n",
    "post_training_analysis(\"irrelevant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
